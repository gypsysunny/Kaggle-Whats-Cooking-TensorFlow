{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interesting Facts:**\n",
    "* Deep learning library developed by Google Brain Team\n",
    "* Computationaly efficient (eliminates GPU overhead by performing computations *entirely* outside of Python)\n",
    "* Includes a visualization board\n",
    "* Well documented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What it does:**\n",
    "* Graph as a data structure\n",
    "  * Computations are represented as graphs \n",
    "  * Information is stores inside a graph (TensorFlow variables)\n",
    "* Executes computations inside Session instances\n",
    "  * Feeds / Fetches to retreive results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Workflow phases:**\n",
    "* Construction Phase\n",
    "  * Encode data as a one dimensional vector\n",
    "  * Define structure Variables\n",
    "* Execution Phase\n",
    "  * Initiate Interactive Session\n",
    "  * Train the model\n",
    "  * Retrieve results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor** is a **multi-dimensional array**. A TensorFlow tensor as an n-dimensional array or list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nodes** in the graph are called **ops** (short for operations). An op takes zero or more Tensors, performs some computation, and produces zero or more Tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A TensorFlow graph is a description of computations. To compute anything, a graph must be launched in a Session. A Session places the graph ops onto Devices, such as CPUs or GPUs, and provides methods to execute them. These methods return tensors produced by ops as numpy ndarray objects in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "39774 of recipes, 7126 unique ingredients, 20 cuisines.\n",
    "\n",
    "Redundant ingredients, ingredients name not standardized (brands, measurements, types)\n",
    "\n",
    "* Every recipe and cuisine must be converted to One-Hot Vectors\n",
    "\n",
    "* [0,0,0,1,0,0,1]\n",
    "\n",
    "* Need two tensors, one for ingredients and one for cuisine labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_file = \"train.json\"\n",
    "\n",
    "with open(train_file, 'r') as fh:\n",
    "    train_ln = json.load(fh)\n",
    "    \n",
    "test_file = \"test.json\"\n",
    "\n",
    "with open(test_file, 'r') as fh:\n",
    "    test_ln = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cuisines_train = list()\n",
    "ingredients_train = list()\n",
    "for i in train_ln:\n",
    "    cuisines_train.append(i['cuisine'])\n",
    "    ingredients_train.append([i.lower() for i in i['ingredients']])\n",
    "    \n",
    "ingredients_test = list()\n",
    "for i in test_ln:\n",
    "    ingredients_test.append([i.lower() for i in i['ingredients']])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39774\n",
      "39774\n",
      "9944\n"
     ]
    }
   ],
   "source": [
    "print len(ingredients_train)\n",
    "print len(cuisines_train)\n",
    "print len(ingredients_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep track of test ids\n",
    "test_ids = [i['id'] for i in test_ln]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bear\n",
      "[u'pancetta', u'vegetable oil', u'spelt flour', u'egg yolks', u'salt', u'onions', u'black pepper', u'buttermilk', u'garlic cloves', u'bear', u'all-purpose flour']\n",
      "2647\n"
     ]
    }
   ],
   "source": [
    "# Difference between train and test sets\n",
    "diff_ingr = list(set([item for sub in ingredients_train for item in sub]) - set([item for sub in ingredients_test for item in sub]))\n",
    "for i in diff_ingr:\n",
    "    if i == 'bear':\n",
    "        print i\n",
    "        \n",
    "# Bear Recipe\n",
    "\n",
    "for i in ingredients_train:\n",
    "    for j in i:\n",
    "        if j == 'bear':\n",
    "            print i\n",
    "            \n",
    "print len(diff_ingr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7126\n"
     ]
    }
   ],
   "source": [
    "# Get unique values for cuisine and ingredients\n",
    "cuisines_set = set(cuisines_train)\n",
    "ingredients_set = set([item for sub in ingredients_train for item in sub] + [item for sub in ingredients_test for item in sub])\n",
    "print len(ingredients_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create binary mapping for each ingredient and cuisine\n",
    "\n",
    "ingredient_mapper = dict()\n",
    "for count, elem in enumerate(ingredients_set):\n",
    "    ingredient_mapper[elem] = count\n",
    "    \n",
    "cuisines_mapper = dict()\n",
    "for count, elem in enumerate(cuisines_set):\n",
    "    cuisines_mapper[elem] = count\n",
    "\n",
    "# Save keys in a list\n",
    "ingredients_keys = ingredient_mapper.keys()\n",
    "cuisines_keys = cuisines_mapper.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_ingredient(ingredient_list):\n",
    "    encoded_ingr = np.zeros(len(ingredients_set))\n",
    "    for ing in ingredient_list:\n",
    "        index = ingredient_mapper[ing]\n",
    "        encoded_ingr[index] = 1\n",
    "    return encoded_ingr\n",
    "\n",
    "def encode_cuisine(cuisine_list):\n",
    "    encoded_cuis = np.zeros(len(cuisines_set))\n",
    "    if i in cuisines_keys:\n",
    "        encoded_cuis[cuisines_mapper[i]] = 1\n",
    "    return encoded_cuis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Produce encoded 7126-dimensional vector for ingredients (train)\n",
    "\n",
    "encoded_ingredients_list_train = list()\n",
    "for i in ingredients_train:\n",
    "    encoded_ingredients_list_train.append(encode_ingredient(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Produce encoded 20-dimensional vector for cuisines\n",
    "\n",
    "encoded_cuisines_list_train = list()\n",
    "for i in cuisines_train:\n",
    "    encoded_cuisines_list_train.append(encode_cuisine(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Produce encoded 7126-dimensional vector for ingredients (test)\n",
    "\n",
    "encoded_ingredients_list_test = list()\n",
    "for i in ingredients_test:\n",
    "    encoded_ingr = np.zeros(len(ingredients_set))\n",
    "    for j in i:\n",
    "        index = ingredient_mapper[j]\n",
    "        encoded_ingr[index] = 1\n",
    "    encoded_ingredients_list_test.append(encoded_ingr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to Numpy Array\n",
    "\n",
    "ing_train = [np.asarray(i) for i in encoded_ingredients_list_train]\n",
    "cuis_train = [np.asarray(i) for i in encoded_cuisines_list_train]\n",
    "ing_test = [np.asarray(i) for i in encoded_ingredients_list_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39774\n",
      "39774\n",
      "9944\n",
      "39774\n",
      "39774\n",
      "9944\n",
      "7126\n"
     ]
    }
   ],
   "source": [
    "print len(encoded_ingredients_list_train)\n",
    "print len(encoded_cuisines_list_train)\n",
    "print len(encoded_ingredients_list_test)\n",
    "print len(ing_train)\n",
    "print len(cuis_train)\n",
    "print len(ing_test)\n",
    "print len(ing_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Must declare variables.\n",
    "* We actually just defining things, no computation is performed.\n",
    "* Constants, variables that only pass output and don't have any input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Placeholder variables are for input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diag.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 7126])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Variables are for internal computation\n",
    "* Store all the computational information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([7126, 20]))\n",
    "b = tf.Variable(tf.zeros([20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/comp.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "y_ = tf.placeholder(tf.float32, [None, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function is defined as cross_entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Determine parameters through back propagation\n",
    "* Define the learning steps to minimize the loss by optimizing the learning rate of 0.01 Gradient Descent method.\n",
    "* Backpropagation with Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learning Step\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initiates a session to actually perform computations\n",
    "# Parralelization. We can assign CPUs/GPUs here\n",
    "sess = tf.InteractiveSession()\n",
    "# Initiate our variables\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Close the session if running and recomputation is needed\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We we actually training model\n",
    "* Build model and slowly adding sets of K random observations\n",
    "* Play around with number of passes and batch sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network does not follow a linear path. Information is processed collectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(4000):\n",
    "    K = 30\n",
    "    indices = random.sample(range(len(ing_train)), K)\n",
    "    batch_xs = [ing_train[i] for i in sorted(indices)]\n",
    "    batch_ys = [cuis_train[i] for i in sorted(indices)]\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterations: 1000, K = 100, Training Accuracy: 75%\n",
    "\n",
    "Iterations: 1000, K = 150, Training Accuracy: 76.3%\n",
    "\n",
    "Iterations: 2000, K = 100, Training Accuracy: 79%\n",
    "\n",
    "Iterations: 2000, K = 150, Training Accuracy: 81.2%\n",
    "\n",
    "Iterations: 3000, K = 100, Training Accuracy: 82%\n",
    "\n",
    "Iterations: 3000, K = 150, Training Accuracy: 81.1%\n",
    "\n",
    "Iterations: 4000, K = 100, Training Accuracy: 80.7\n",
    "\n",
    "Iterations: 4000, K = 150, Training Accuracy: 84.4%\n",
    "\n",
    "Iterations: 4000, K = 50, Training Accuracy: 85.0%\n",
    "\n",
    "Iterations: 4000, K = 30, Training Accuracy: 85.5% Testing Accuracy: 0.77484\n",
    "\n",
    "Iterations: 5000, K = 100, Training Accuracy: 81.5\n",
    "\n",
    "Iterations: 5000, K = 50, Training Accuracy: 86.4%\n",
    "\n",
    "Iterations: 5000, K = 50, Training Accuracy: 86.4%\n",
    "\n",
    "Iterations: 6000, K = 100, Training Accuracy: 85.5\n",
    "\n",
    "Iterations: 6000, K = 100, Training Accuracy: 84 Testing Accuracy: 0.77233\n",
    "\n",
    "Iterations: 10000, K = 50, Training Accuracy: 81.5 Testing Accuracy: 0.76478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a separate instance of interactive session for each computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sess.run(accuracy, feed_dict={x: ing_train, y_: cuis_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve prediction for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = tf.argmax(y,1).eval(feed_dict={x: ing_test, keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cuis_pred = list()\n",
    "for i in prediction:\n",
    "    for name, num in cuisines_mapper.iteritems():\n",
    "        if num == i:\n",
    "            cuis_pred.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dictionary = dict(zip(test_ids, cuis_pred))\n",
    "\n",
    "df = pd.DataFrame(dictionary.items())\n",
    "df.columns = ['id', 'cuisine']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"top.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"me.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
